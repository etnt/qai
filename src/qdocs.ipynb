{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we need to augment the models through some external means.\\nIn this section we first provide a brief overview of the main\\nshortcoming of LLMs, with a deeper look at the issue of\\nhallucination. We then describe how prompting and some aug-\\nmentation approaches can not only address those limitations\\nbut also be used to augment the capabilities of LLMs going\\nas far as turning an LLM into a full-blown AI agent with the\\nability to interface with the external world.\\nA. LLM limitations\\nIt is important to remember that LLMs are trained to predict\\na token. While fine-tuning and alignment improves their per-\\nformance and adds different dimensions to their abilities, there\\nare still some important limitations that come up, particularly\\nif they are used naively. Some of them include the following:\\n• They don’t have state/memory. LLMs on their own\\ncannot remember even what was sent to them in the\\nprevious prompt. That is an important limitation for\\nmany of the uses cases that require some form of state.\\n• They are stochastic/probabilistic. If you send the same\\nprompt to an LLM several times, you are likely to get\\ndifferent responses. While there are parameters, and\\nin particular the temperature, to limit the variability\\nin the response, this is an inherent property of their\\ntraining that can create issues.\\n• They have stale information and, on their own, don’t\\nhave access to external data. An LLM on its own does\\nnot even know about the current time or day and does\\nnot have access to any information that was not present\\nin its training set.\\n• They are generally very large. This means that many\\ncostly GPU machines are needed for training and\\nserving. In some cases, largest models have poor\\nSLAs, particularly in terms of latency.\\n• They hallucinate. LLMs do not have a notion of\\n”truth” and they have usually been trained on a mix\\nof good and bad content. They can produce very\\nplausible but untruthful answers.\\nWhile the previous limitations can all become important\\nfor some applications, it is worth for us to dive a bit into the\\nlast one, hallucinations, since it has gathered a lot of interest\\nover the past few months and it has also sparked many of the\\nprompt approaches and LLM augmentation methods we will\\nlater describe.\\nHallucination: In the realm of Large Language Models\\n(LLMs), the phenomenon of ”hallucinations” has garnered\\nsignificant attention. Defined in the literature, notably in the\\n”Survey of Hallucination in Natural Language Generation”\\npaper [145], hallucination in an LLM is characterized as\\n”the generation of content that is nonsensical or unfaithful\\nto the provided source.” This terminology, although rooted in\\npsychological parlance, has been appropriated within the field\\nof artificial intelligence.\\nHallucinations in LLMs can be broadly categorized into\\ntwo types:1) Intrinsic Hallucinations : These directly conflict with\\nthe source material, introducing factual inaccuracies\\nor logical inconsistencies.\\n2) Extrinsic Hallucinations : These, while not contra-\\ndicting, are unverifiable against the source, encom-\\npassing speculative or unconfirmable elements.\\nThe definition of ’source’ in LLM contexts varies with the\\ntask. In dialogue-based tasks, it refers to ’world knowledge’,\\nwhereas in text summarization, it pertains to the input text\\nitself. This distinction plays a crucial role in evaluating and\\ninterpreting hallucinations. The impact of hallucinations is also\\nhighly context-dependent. For instance, in creative endeavors\\nlike poem writing, hallucinations might be deemed acceptable\\nor even beneficial.\\nLLMs, trained on diverse datasets including the internet,\\nbooks, and Wikipedia, generate text based on probabilistic\\nmodels without an inherent understanding of truth or falsity.\\nRecent advancements like instruct tuning and Reinforcement\\nLearning from Human Feedback (RLHF) have attempted to\\nsteer LLMs towards more factual outputs, but the fundamental\\nprobabilistic nature and its inherent limitations remain. A\\nrecent study, “Sources of Hallucination by Large Language\\nModels on Inference Tasks” [146], highlights two key aspects\\ncontributing to hallucinations in LLMs: the veracity prior and\\nthe relative frequency heuristic, underscoring the complexities\\ninherent in LLM training and output generation.\\nEffective automated measurement of hallucinations in\\nLLMs requires a combination of statistical and model-based\\nmetrics.\\nStatistical Metrics :\\n• Metrics like ROUGE [147] and BLEU [148] are com-\\nmon for assessing text similarity, focusing on intrinsic\\nhallucinations.\\n• Advanced metrics such as PARENT [149], PARENT-\\nT [150], and Knowledge F1 [151] are utilized when\\nstructured knowledge sources are available. These\\nmetrics, while effective, have limitations in capturing\\nsyntactic and semantic nuances.\\nModel-Based Metrics :\\n• IE-Based Metrics : Utilize Information Extraction\\nmodels to simplify knowledge into relational tuples,\\nthen compare these with the source.\\n• QA-Based Metrics : Assess the overlap between gen-\\nerated content and the source through a question-\\nanswering framework (see [152]).\\n• NLI-Based Metrics : Use Natural Language Inference\\ndatasets to evaluate the truthfulness of a generated\\nhypothesis based on a given premise (see [153]).\\n• Faithfulness Classification Metrics : Offer a refined\\nassessment by creating task-specific datasets for a\\nnuanced evaluation (see [154]).\\nDespite advances in automated metrics, human judgment\\nremains a vital piece. It typically involves two methodologies:'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_file = \"../data/LLM-Overview.pdf\"\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "pdf_reader.pages[20].extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Split up the PDF document into Pages.\n",
    "docs = []\n",
    "for page_index, page in enumerate(pdf_reader.pages):\n",
    "        docs.append(Document(page_content=page.extract_text(), meta_data={'file': pdf_file, 'page': page_index}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='GPT Family PaLM Family    LLaMA  1/2 Family\\nGPT\\nGPT1\\nGPT2\\nGPT3\\nGPT4\\nGPT3.5 Turbo\\ntext-davinci\\ncode-davinci\\nCODEX\\nInstructGPT\\nWebGPT\\nGPT4 V ision\\nGPT4 Turbo\\nGorilla\\nMistral\\nVigogne\\nStable Beluga2\\nKoala\\nCode LLaMA\\nVicuna\\n Alpaca\\nBaize\\nLong LLaMA\\nGiraf fe\\nGuanaco\\nTulu\\nWizardLM\\nMed-PaLM\\nPaLM-E\\n Med-PaLM2\\nFLAN-PaLM\\n U-PaLM\\nPaLM2\\nPaLM\\nFig. 8: Popular LLM Families.\\nFig. 9: GPT-3 shows that larger models make increasingly\\nefficient use of in-context information. It shows in-context\\nlearning performance on a simple task requiring the model to\\nremove random symbols from a word, both with and without\\na natural language task description. Courtesy of [56].\\n(RLHF), as shown in 10. The resultant InstructGPT models\\nhave shown improvements in truthfulness and reductions in\\ntoxic output generation while having minimal performance\\nregressions on public NLP datasets.\\nFig. 10: The high-level overview of RLHF. Courtesy of [59].\\nThe most important milestone of LLM development is thelaunch of ChatGPT (Chat Generative Pre-trained Transformer)\\n[60] on November 30, 2022. ChatGPT is chatbot that enables\\nusers to steer a conversation to complete a wide range of\\ntasks such as question answering, information seeking, text\\nsummarization, and more. ChatGPT is powered by GPT-3.5\\n(and later by GPT-4), a sibling model to InstructGPT, which\\nis trained to follow an instruction in a prompt and provide a\\ndetailed response.\\nGPT-4 [33] is the latest and most powerful LLM in the\\nGPT family. Launched in March, 2023, GPT-4 is a multi-\\nmodal LLM in that it can take image and text as inputs and\\nproduce text outputs. While still less capable than humans\\nin some of the most challenging real-world scenarios, GPT-4\\nexhibits human-level performance on various professional and\\nacademic benchmarks, including passing a simulated bar exam\\nwith a score around the top 10% of test takers, as shown in\\nFig 11. Like early GPT models, GPT-4 was first pre-trained to\\npredict next tokens on large text corpora, and then fine-tuned\\nwith RLHF to align model behaviors with human-desired ones.\\n2)The LLaMA Family :LLaMA is a collection of founda-\\ntion language models, released by Meta. Unlike GPT models,\\nLLaMA models are open-source, i.e., model weights are\\nreleased to the research community under a noncommercial\\nlicense. Thus, the LLaMA family grows rapidly as these\\nmodels are widely used by many research groups to develop\\nbetter open-source LLMs to compete the closed-source ones or\\nto develop task-specific LLMs for mission-critical applications.\\nThe first set of LLaMA models [32] was released in Febru-\\nary 2023, ranging from 7B to 65B parameters. These models\\nare pre-trained on trillions of tokens, collected from publicly\\navailable datasets. LLaMA uses the transformer architecture of\\nGPT-3, with a few minor architectural modifications, including\\n(1) using a SwiGLU activation function instead of ReLU,\\n(2) using rotary positional embeddings instead of absolute\\npositional embedding, and (3) using root-mean-squared layer-\\nnormalization instead of standard layer-normalization. The\\nopen-source LLaMA-13B model outperforms the proprietary\\nGPT-3 (175B) model on most benchmarks, making it a good\\nbaseline for LLM research.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large Language Models: A Survey\\nShervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu\\nRichard Socher, Xavier Amatriain, Jianfeng Gao\\nAbstract —Large Language Models (LLMs) have drawn a\\nlot of attention due to their strong performance on a wide\\nrange of natural language tasks, since the release of ChatGPT\\nin November 2022. LLMs’ ability of general-purpose language\\nunderstanding and generation is acquired by training billions of\\nmodel’s parameters on massive amounts of text data, as predicted\\nby scaling laws [1], [2]. The research area of LLMs, while very\\nrecent, is evolving rapidly in many different ways. In this paper,\\nwe review some of the most prominent LLMs, including three\\npopular LLM families (GPT, LLaMA, PaLM), and discuss their\\ncharacteristics, contributions and limitations. We also give an\\noverview of techniques developed to build, and augment LLMs.\\nWe then survey popular datasets prepared for LLM training,\\nfine-tuning, and evaluation, review widely used LLM evaluation\\nmetrics, and compare the performance of several popular LLMs\\non a set of representative benchmarks. Finally, we conclude\\nthe paper by discussing open challenges and future research\\ndirections.\\nI. I NTRODUCTION\\nLanguage modeling is a long-standing research topic, dat-\\ning back to the 1950s with Shannon’s application of informa-\\ntion theory to human language, where he measured how well\\nsimple n-gram language models predict or compress natural\\nlanguage text [3]. Since then, statistical language modeling\\nbecame fundamental to many natural language understanding\\nand generation tasks, ranging from speech recognition, ma-\\nchine translation, to information retrieval [4], [5], [6].\\nThe recent advances on transformer-based large language\\nmodels (LLMs), pretrained on Web-scale text corpora, signif-\\nicantly extended the capabilities of language models (LLMs).\\nFor example, OpenAI’s ChatGPT and GPT-4 can be used not\\nonly for natural language processing, but also as general task\\nsolvers to power Microsoft’s Co-Pilot systems, for instance,\\ncan follow human instructions of complex new tasks per-\\nforming multi-step reasoning when needed. LLMs are thus\\nbecoming the basic building block for the development of\\ngeneral-purpose AI agents or artificial general intelligence\\n(AGI).\\nAs the field of LLMs is moving fast, with new findings,\\nmodels and techniques being published in a matter of months\\nor weeks [7], [8], [9], [10], [11], AI researchers and practi-\\ntioners often find it challenging to figure out the best recipes\\nto build LLM-powered AI systems for their tasks. This paper\\ngives a timely survey of the recent advances on LLMs. We\\nhope this survey will prove a valuable and accessible resource\\nfor students, researchers and developers.\\nLLMs are large-scale, pre-trained, statistical language mod-\\nels based on neural networks. The recent success of LLMs is\\nan accumulation of decades of research and development of\\nlanguage models, which can be categorized into four wavesthat have different starting points and velocity: statistical lan-\\nguage models, neural language models, pre-trained language\\nmodels and LLMs.\\nStatistical language models (SLMs) view text as a sequence\\nof words, and estimate the probability of text as the product\\nof their word probabilities. The dominating form of SLMs\\nare Markov chain models known as the n-gram models,\\nwhich compute the probability of a word conditioned on its\\nimmediate proceeding n−1words. Since word probabilities\\nare estimated using word and n-gram counts collected from\\ntext corpora, the model needs to deal with data sparsity (i.e.,\\nassigning zero probabilities to unseen words or n-grams) by\\nusing smoothing , where some probability mass of the model\\nis reserved for unseen n-grams [12]. N-gram models are\\nwidely used in many NLP systems. However, these models\\nare incomplete in that they cannot fully capture the diversity\\nand variability of natural language due to data sparsity.\\nEarly neural language models (NLMs) [13], [14], [15], [16]\\ndeal with data sparsity by mapping words to low-dimensional\\ncontinuous vectors (embedding vectors) and predict the next\\nword based on the aggregation of the embedding vectors of\\nits proceeding words using neural networks. The embedding\\nvectors learned by NLMs define a hidden space where the\\nsemantic similarity between vectors can be readily computed\\nas their distance. This opens the door to computing semantic\\nsimilarity of any two inputs regardless their forms (e.g., queries\\nvs. documents in Web search [17], [18], sentences in different\\nlanguages in machine translation [19], [20]) or modalities (e.g.,\\nimage and text in image captioning [21], [22]). Early NLMs are\\ntask-specific models, in that they are trained on task-specific\\ndata and their learned hidden space is task-specific.\\nPre-trained language models (PLMs), unlike early NLMs,\\nare task-agnostic. This generality also extends to the learned\\nhidden embedding space. The training and inference of PLMs\\nfollows the pre-training and fine-tuning paradigm, where lan-\\nguage models with recurrent neural networks [23] or trans-\\nformers [24], [25], [26] are pre-trained on Web-scale unlabeled\\ntext corpora for general tasks such as word prediction, and then\\nfinetuned to specific tasks using small amounts of (labeled)\\ntask-specific data. Recent surveys on PLMs include [8], [27],\\n[28].\\nLarge language models (LLMs) mainly refer to\\ntransformer-based neural language models1that contain\\ntens to hundreds of billions of parameters, which are pre-\\ntrained on massive text data, such as PaLM [31], LLaMA\\n[32], and GPT-4 [33], as summarized in Table III. Compared\\n1Recently, several very promising non-transformer LLMs have been pro-\\nposed, such as the LLMs based on structured state space models [29], [30].\\nSee Section VII for more details.arXiv:2402.06196v2  [cs.CL]  20 Feb 2024'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "# Build up a list of Documents, each holding a Paragraph from a Page.\n",
    "# Meta-data identifies the source and the page number of the Document.\n",
    "doc_texts = []\n",
    "for pindex, page in enumerate(docs):    \n",
    "    doc_texts.extend(text_splitter.create_documents(texts=[page.page_content], metadatas=[{'file': pdf_file, 'page': pindex}]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': '../data/LLM-Overview.pdf', 'page': 8}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_texts[200].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qutils import VectorStore\n",
    "\n",
    "# Prepare for our persistent Vector Store if non-existent.\n",
    "# Else open the existing Vector Store.\n",
    "v = VectorStore(persist_directory=\"DocsDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DocsDB'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no Vector Store exist, create one and load the documents.\n",
    "if not v:\n",
    "    v.store_documents(doc_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for text chunks similar to our Query.\n",
    "answer = v.similarity_search(\n",
    "    query=\"Why is the Transformer so successful\",\n",
    "    num_results=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# Find out what Pages the text chunks are referring to.\n",
    "s = set()\n",
    "for a in answer:\n",
    "    s.add(a.metadata['page'])\n",
    "    print(a.metadata['page'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{11, 39}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
