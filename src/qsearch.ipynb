{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    TYPE_CHECKING,\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Iterable,\n",
    "    List,\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    Type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qvdb import VectorDB\n",
    "\n",
    "qvdb = VectorDB(is_persistent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import warnings\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Suppress warnings about insecure HTTPS requests.\n",
    "warnings.simplefilter('ignore', InsecureRequestWarning)\n",
    "\n",
    "def extract_url_content(url: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extracts the content from the given URL.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL to extract content from.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted content.\n",
    "        \"\"\"\n",
    "        response = requests.get(url, verify=False)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text = ' '.join([p.text for p in soup.find_all('p')])  # Extract text from <p> tags\n",
    "\n",
    "        # We need to split it up into smaller pieces\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        text_splits = text_splitter.split_text(text)\n",
    "\n",
    "        return text_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "\n",
    "def search_and_store(qvdb: VectorDB, query: str, num_results: int = 5) -> None:\n",
    "        \"\"\"\n",
    "        Performs a Google Search for documents related to the given query,\n",
    "        extracts their content, and stores them in the vector store.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query to search for documents.\n",
    "            num_results (int): The number of URLs to retrieve. Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for index, url in enumerate(search(query, num_results)):\n",
    "            print(f\"<info> ({index}) Extracting content from: {url}\")\n",
    "            text_splits = extract_url_content(url=url)\n",
    "            for jindex, text in enumerate(text_splits):\n",
    "                qvdb.add(\n",
    "                    documents=[text],\n",
    "                    metadatas=[{\"source\": url}],\n",
    "                    ids=[f\"id{index}.{jindex}\"]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = 'mistral'\n",
    "model= 'llama3'\n",
    "system=\"\"\"\n",
    "You are a helpful AI assistant that helps answer questions based on documents and sources.\n",
    "You may combine your own knowledge with the information in the documents to answer the questions.\n",
    "If you make use of the provided documents then add the sources to the answer.\n",
    "Try to keep the column width of the response to 72 characters.\n",
    "\n",
    "Example of the documents:\n",
    "\n",
    "SOURCE: https://www.example.com\n",
    "DOCUMENT: This is an example document.\n",
    "\n",
    "\"\"\"\n",
    "template = \"\"\"\n",
    "Here is the question: {question}\n",
    "\n",
    "Here are the documents and the corresponding sources:\n",
    "{documents}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = \"What makes Virus different from Bacteria?\"\n",
    "#question = \"How do I print just a few lines of a file using the sed command?\"\n",
    "#question = \"Elaborate on who came up with the Periodical System?\"\n",
    "#question = \"With the increasing problem of antibiotic resistance, what are the alternatives to antibiotics?\"\n",
    "#question = \"Who invented the Erlang programming language and what was the reason behind it?\"\n",
    "question = \"What are the main differences between the GPT and the BERT models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<info> (0) Extracting content from: https://blog.invgate.com/gpt-3-vs-bert\n",
      "<info> (1) Extracting content from: https://softteco.com/blog/bert-vs-chatgpt\n",
      "<info> (2) Extracting content from: https://medium.com/@prudhvithtavva/bert-vs-gpt-a-tale-of-two-transformers-that-revolutionized-nlp-11fff8e61984\n",
      "<info> (3) Extracting content from: https://www.baeldung.com/cs/bert-vs-gpt-3-architecture\n",
      "<info> (4) Extracting content from: https://medium.com/@reyhaneh.esmailbeigi/bert-gpt-and-bart-a-short-comparison-5d6a57175fca\n",
      "<info> (5) Extracting content from: https://symbl.ai/developers/blog/gpt-3-versus-bert-a-high-level-comparison/\n",
      "<info> (6) Extracting content from: https://www.quora.com/What-are-the-differences-between-Googles-Bert-and-OpenAIs-GPT-2-artificial-intelligence-models\n",
      "{'ids': [['id0.4', 'id0.5', 'id0.3', 'id1.17', 'id1.18']], 'distances': [[0.17108333110809326, 0.19292718172073364, 0.2135852575302124, 0.21542876958847046, 0.21935486793518066]], 'metadatas': [[{'source': 'https://blog.invgate.com/gpt-3-vs-bert'}, {'source': 'https://blog.invgate.com/gpt-3-vs-bert'}, {'source': 'https://blog.invgate.com/gpt-3-vs-bert'}, {'source': 'https://softteco.com/blog/bert-vs-chatgpt'}, {'source': 'https://softteco.com/blog/bert-vs-chatgpt'}]], 'embeddings': None, 'documents': [['phrase is essential. Another difference between the two models lies in their training datasets. While both models were trained on large datasets of text data from sources like Wikipedia and books, GPT-3 was trained on 45TB of data, while BERT was trained on 3TB of data. So, GPT-3 has access to more information than BERT, which could give it an edge in specific tasks such as summarization or translation, where access to more data can be beneficial. Finally, there are differences in terms of size', 'Finally, there are differences in terms of size as well. While both models are very large (GPT-3 has 1.5 billion parameters while BERT has 340 million parameters), GPT-3 is significantly larger than its predecessor due to its much more extensive training dataset size (470 times bigger than the one used to train BERT). Despite their differences in architecture and training datasets size, there are also some similarities between GPT-3 and BERT: Both GPT-3 and BERT have been shown to perform well', 'the base for a number of services, like: The most obvious difference between GPT-3 and BERT is their architecture. As mentioned above, GPT-3 is an autoregressive model, while BERT is bidirectional. While GPT-3 only considers the left context when making predictions, BERT takes into account both left and right context. This makes BERT better suited for tasks such as sentiment analysis or NLU, where understanding the full context of a sentence or phrase is essential. Another difference between', 'It means GPT-3 is significantly larger than its competitor due to its much more extensive training dataset size. GPT-3 is typically fine-tuned on specific tasks during training with task-specific examples. It can be fine-tuned for various tasks by using small datasets. BERT is pre-trained on a large dataset and then fine-tuned on specific tasks. It requires training datasets tailored to particular tasks for effective performance. To answer the question which model is better, BERT vs. GPT-3,', 'question which model is better, BERT vs. GPT-3, weâ€™ve compiled all the main information in a brief comparison table.\\xa0 BERT and GPT-3 language models are tangible examples of what AI is capable of and we have already benefited from them in real life. However, as these models evolve and become more intelligent, it is critical to keep in mind their limitations and pitfalls, which are and will be present. Hence, people can delegate some of their responsibilities to AI and use language models as']], 'uris': None, 'data': None}\n"
     ]
    }
   ],
   "source": [
    "search_and_store(qvdb=qvdb, query=question, num_results=5)\n",
    "results = qvdb.query(question, num_results=5)\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]\n",
    "for d, s in zip(results['documents'][0], results['metadatas'][0]):\n",
    "    docs.append(f\"SOURCE: {s['source']}\\nDOCUMENT: {d}\\n\")\n",
    "documents = ' '.join(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "prompt = template.format(question=question, documents=documents)\n",
    "output = ollama.generate(model=model, system=system, prompt=prompt, stream=False)\n",
    "response = output['response'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main differences between GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) models are:\n",
      "\n",
      "* Training datasets: GPT-3 was trained on 45TB of data, while BERT was trained on 3TB of data.\n",
      "* Architecture: GPT-3 is an autoregressive model, while BERT is bidirectional. GPT-3 only considers the left context when making predictions, whereas BERT takes into account both left and right context.\n",
      "* Size: GPT-3 has 1.5 billion parameters, while BERT has 340 million parameters. GPT-3 is significantly larger than its predecessor due to its much more extensive training dataset size.\n",
      "\n",
      "Sources:\n",
      "https://blog.invgate.com/gpt-3-vs-bert\n",
      "https://softteco.com/blog/bert-vs-chatgpt\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvdb.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
